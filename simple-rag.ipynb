{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f61ebd-c891-4220-a977-9236700ceec7",
   "metadata": {},
   "source": [
    "# PDF Querying with ChromaDB and ChatGPT\n",
    "\n",
    "> PyCon Ireland 2024\n",
    "\n",
    "## Overview\n",
    "\n",
    "This script processes a PDF document, extracts its content, stores it in ChromaDB.\n",
    "It uses a language model (ChatGPT or Ollama) to generate context-aware responses.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **PDF Ingestion**:\n",
    "   - Downloads a sample PDF if not already available.\n",
    "   - Extracts text from the PDF and splits it into manageable chunks.\n",
    "\n",
    "2. **ChromaDB Setup**:\n",
    "   - Initializes a ChromaDB client and creates or retrieves a collection.\n",
    "   - Stores the text chunks in the ChromaDB collection using `upsert`.\n",
    "\n",
    "3. **Querying**:\n",
    "   - Searches the ChromaDB collection for chunks most similar to the userâ€™s query.\n",
    "   - Passes the top match as context to the selected language model (ChatGPT or Ollama).\n",
    "\n",
    "4. **Response Generation**:\n",
    "   - The language model generates a detailed response based on the retrieved context and user query.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Configurable**: Supports ChatGPT (via OpenAI API) and Ollama (local model).\n",
    "- **Vector-Based Search**: Efficient querying using semantic similarity.\n",
    "- **Contextual Responses**: Provides answers tailored to the retrieved PDF content.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.8 or higher\n",
    "- OpenAI API key (if using ChatGPT)\n",
    "- ChromaDB installed (`pip install chromadb`)\n",
    "- Optional: Ollama running locally for local LLM support\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. Set up the constants in the script, including `QUERY` and `MODEL_TYPE`.\n",
    "2. Run the script.\n",
    "3. Inspect the results:\n",
    "   - View top matches from the vector database.\n",
    "   - Get a detailed response from the language model based on the query\n",
    "\n",
    "## Workflow Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Start] --> B[Download Sample PDF]\n",
    "    B --> C[Extract Text from PDF]\n",
    "    C --> D[Split Text into Chunks]\n",
    "    D --> E[Initialize ChromaDB Client]\n",
    "    E --> F[Store Chunks in ChromaDB Collection]\n",
    "    F --> G[Query Vector DB with User Query]\n",
    "    G --> H[Retrieve Top Matches]\n",
    "    H --> I[Pass Top Match to LLM]\n",
    "    I --> J[Generate Response]\n",
    "    J --> K[Display Results]\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8c57b-24cb-4cf7-99d4-af1df63198c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install chromadb openai PyPDF2 sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132068c6-7088-4602-8aaf-4aa2cba7307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List\n",
    "from PyPDF2 import PdfReader\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import requests\n",
    "\n",
    "# -----------------------------------------\n",
    "# Setup Logging\n",
    "# -----------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da989d4c-03a4-41af-a3ba-86aa0fb2e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Constants: Configure the script here\n",
    "# -----------------------------------------\n",
    "# Sample PDF URL to download if the file doesn't exist locally\n",
    "SAMPLE_PDF_URL = \"https://pdfobject.com/pdf/sample.pdf\"\n",
    "\n",
    "# Path to save the sample PDF file\n",
    "PDF_PATH = \"sample.pdf\"  # Local file name for the PDF\n",
    "\n",
    "# Default query for testing\n",
    "QUERY = \"What does this document talk about?\"  # Example query\n",
    "\n",
    "# Directory where ChromaDB will persist its data\n",
    "CHROMA_DB_DIR = \"./chromadb\"  # Path to ChromaDB storage\n",
    "\n",
    "# Name of the ChromaDB collection to create or retrieve\n",
    "COLLECTION_NAME = \"pdf_documents\"  # Logical name for the collection\n",
    "\n",
    "# Maximum size of text chunks when splitting the PDF content\n",
    "CHUNK_SIZE = 500  # Number of characters per text chunk\n",
    "\n",
    "# Configurable model selection: \"chatgpt\" or \"ollama\"\n",
    "MODEL_TYPE = \"ollama\"  # Default is Ollama; change to \"chatgpt\" for OpenAI's GPT\n",
    "\n",
    "# Ollama-specific configurations\n",
    "OLLAMA_ENDPOINT = \"http://localhost:11434/api/generate\"  # Local Ollama server endpoint\n",
    "DEFAULT_OLLAMA_MODEL = \"phi3.5:latest\"  # Default Ollama model\n",
    "\n",
    "# OpenAI-specific configurations\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"your_openai_api_key\")  # Replace with your OpenAI API key\n",
    "CHATGPT_MODEL_NAME = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa535c-cfff-42d0-860c-e2914a2c34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Utility Functions\n",
    "# -----------------------------------------\n",
    "def download_sample_pdf(url: str, save_path: str) -> None:\n",
    "    \"\"\"Downloads a sample PDF file from a given URL.\"\"\"\n",
    "    logger.info(\"Downloading sample PDF...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    logger.info(f\"Sample PDF downloaded to {save_path}\")\n",
    "\n",
    "\n",
    "def verify_pdf_file(file_path: str) -> None:\n",
    "    \"\"\"Verifies that the specified PDF file exists.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found at {file_path}.\")\n",
    "    logger.info(f\"PDF file verified at {file_path}\")\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts all text from a PDF file.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    logger.info(\"Text extracted from PDF.\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int = 500) -> List[str]:\n",
    "    \"\"\"Splits a large text into smaller chunks for embedding.\"\"\"\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    logger.info(f\"Text split into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_chroma_client_and_collection(collection_name: str) -> chromadb.api.Collection:\n",
    "    \"\"\"Creates a ChromaDB client and retrieves a collection.\"\"\"\n",
    "    chroma_client = chromadb.Client(Settings(persist_directory=CHROMA_DB_DIR))\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    logger.info(f\"ChromaDB collection '{collection_name}' created or retrieved.\")\n",
    "    return collection\n",
    "\n",
    "\n",
    "def ingest_pdf_to_chromadb(pdf_path: str, collection: chromadb.api.Collection, chunk_size: int = 500) -> None:\n",
    "    \"\"\"Ingests text from a PDF into a ChromaDB collection.\"\"\"\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = split_text_into_chunks(text, chunk_size)\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        if chunk.strip():  # Skip empty chunks\n",
    "            collection.upsert(\n",
    "                documents=[chunk],\n",
    "                ids=[f\"doc_{idx}\"]\n",
    "            )\n",
    "    logger.info(f\"Ingested {len(chunks)} chunks into ChromaDB.\")\n",
    "\n",
    "\n",
    "def query_vector_db(collection: chromadb.api.Collection, query: str, max_results: int = 3) -> List[str]:\n",
    "    \"\"\"Queries the vector database for the most similar chunks.\"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=max_results\n",
    "    )\n",
    "    documents = results[\"documents\"][0]\n",
    "    logger.info(f\"Retrieved {len(documents)} matching chunks from the vector DB.\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Query Functions\n",
    "# -----------------------------------------\n",
    "def query_llm(\n",
    "    query: str,\n",
    "    context: List[str],\n",
    "    model_type: str = MODEL_TYPE,\n",
    "    ollama_model: str = DEFAULT_OLLAMA_MODEL\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Queries a language model (LLM) with additional context.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        context (List[str]): The context retrieved from the vector DB.\n",
    "        model_type (str): The model type to use (\"chatgpt\" or \"ollama\").\n",
    "        ollama_model (str): The specific Ollama model to use.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM response.\n",
    "    \"\"\"\n",
    "    if model_type == \"chatgpt\":\n",
    "        # Initialize the OpenAI client\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "        # Construct the messages with context\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that answers questions about PDF documents.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {' '.join(context)}\\n\\nQuestion: {query}\"}\n",
    "        ]\n",
    "\n",
    "        # Create a chat completion\n",
    "        response = client.chat.completions.create(\n",
    "            model=CHATGPT_MODEL_NAME,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        logger.info(\"ChatGPT query completed.\")\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    elif model_type == \"ollama\":\n",
    "        # Combine context into a single string\n",
    "        combined_context = \" \".join(context)\n",
    "        prompt = f\"Context: {combined_context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "        # Specify the model and send the query to the Ollama server\n",
    "        payload = {\n",
    "            \"model\": ollama_model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False  # Disable streaming for simplicity\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(OLLAMA_ENDPOINT, json=payload)\n",
    "            response.raise_for_status()\n",
    "            logger.info(f\"Ollama query completed using model: {ollama_model}.\")\n",
    "            return response.json().get(\"response\", \"No response received from Ollama.\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            logger.error(f\"HTTPError: {e}\")\n",
    "            logger.error(f\"Response: {e.response.text}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error querying Ollama: {e}\")\n",
    "            raise\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. Use 'chatgpt' or 'ollama'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d1b6bb-1d92-4ecd-89ab-56a561cc6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Main Workflow\n",
    "# -----------------------------------------\n",
    "def main() -> None:\n",
    "    \"\"\"Main workflow to ingest a PDF into ChromaDB, query the vector DB, and query the LLM.\"\"\"\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        download_sample_pdf(SAMPLE_PDF_URL, PDF_PATH)\n",
    "\n",
    "    verify_pdf_file(PDF_PATH)\n",
    "\n",
    "    logger.info(\"Setting up ChromaDB client and collection...\")\n",
    "    collection = create_chroma_client_and_collection(COLLECTION_NAME)\n",
    "\n",
    "    logger.info(f\"Ingesting PDF '{PDF_PATH}' into ChromaDB collection...\")\n",
    "    ingest_pdf_to_chromadb(PDF_PATH, collection, CHUNK_SIZE)\n",
    "\n",
    "    logger.info(\"Querying vector DB...\")\n",
    "    top_matches = query_vector_db(collection, QUERY)\n",
    "\n",
    "    logger.info(\"Top matches from the vector DB:\")\n",
    "    for i, match in enumerate(top_matches, 1):\n",
    "        print(f\"{i}. {match[:200]}...\")  # Display the first 200 characters of each match\n",
    "\n",
    "    logger.info(\"Querying LLM with context from vector DB...\")\n",
    "    response = query_llm(QUERY, top_matches)\n",
    "    logger.info(\"LLM Response:\")\n",
    "    print(response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
