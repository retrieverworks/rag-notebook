{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f61ebd-c891-4220-a977-9236700ceec7",
   "metadata": {},
   "source": [
    "# PDF Querying with ChromaDB and ChatGPT\n",
    "\n",
    "> Build your own \"Chat with Local Files\" using Retrieval Augmented Generation\n",
    "> Mihai Criveti, PyCon Ireland 2024\n",
    "\n",
    "## Overview\n",
    "\n",
    "This script processes a PDF document, extracts its content, stores it in ChromaDB.\n",
    "It uses a language model (FakeLLM, ChatGPT or Ollama) to generate context-aware responses.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **PDF Ingestion**:\n",
    "   - Downloads a sample PDF if not already available.\n",
    "   - Extracts text from the PDF and splits it into chunks.\n",
    "\n",
    "2. **ChromaDB Setup**:\n",
    "   - Initializes a ChromaDB client and creates or retrieves a collection.\n",
    "   - Stores the text chunks in the ChromaDB collection.\n",
    "\n",
    "3. **Querying**:\n",
    "   - Searches the ChromaDB collection for chunks most similar to the user’s query.\n",
    "   - Passes the top match as context to the selected language model (FakeLLM, ChatGPT or Ollama).\n",
    "\n",
    "4. **Response Generation**:\n",
    "   - The language model generates a detailed response based on the retrieved context and user query.\n",
    "\n",
    "## Workflow Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Start] --> B[Download Sample PDF]\n",
    "    B --> C[Extract Text from PDF]\n",
    "    C --> D[Split Text into Chunks]\n",
    "    D --> E[Initialize ChromaDB Client]\n",
    "    E --> F[Store Chunks in ChromaDB Collection]\n",
    "    F --> G[Query Vector DB with User Query]\n",
    "    G --> H[Retrieve Top Matches]\n",
    "    H --> I[Pass Top Match to LLM]\n",
    "    I --> J[Generate Response]\n",
    "    J --> K[Display Results]\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f601da-2a99-4455-bfcc-481154e3020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in ./.venv/lib/python3.11/site-packages (0.5.18)\n",
      "Collecting openai\n",
      "  Downloading openai-1.54.4-py3-none-any.whl (389 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyPDF2 in ./.venv/lib/python3.11/site-packages (3.0.1)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.0-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.7/268.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: build>=1.0.3 in ./.venv/lib/python3.11/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./.venv/lib/python3.11/site-packages (from chromadb) (2.9.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.115.5)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in ./.venv/lib/python3.11/site-packages (from chromadb) (2.1.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.venv/lib/python3.11/site-packages (from chromadb) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (1.28.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.20.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (4.67.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.venv/lib/python3.11/site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (1.68.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.venv/lib/python3.11/site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.13.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in ./.venv/lib/python3.11/site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.venv/lib/python3.11/site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.venv/lib/python3.11/site-packages (from chromadb) (3.10.11)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai) (4.6.2.post1)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch>=1.11.0\n",
      "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.20.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: packaging>=19.1 in ./.venv/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in ./.venv/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in ./.venv/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.41.2)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.36.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (5.28.3)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./.venv/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in ./.venv/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.1 in ./.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.1 in ./.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b1 in ./.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.49b1 in ./.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b1 in ./.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.49b1 in ./.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in ./.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Collecting triton==3.1.0\n",
      "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.11/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./.venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Installing collected packages: triton, threadpoolctl, sympy, scipy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, joblib, jiter, distro, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, openai, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "Successfully installed distro-1.9.0 jiter-0.7.1 joblib-1.4.2 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.54.4 regex-2024.11.6 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.3.0 sympy-1.13.1 threadpoolctl-3.5.0 torch-2.5.1 transformers-4.46.2 triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install chromadb openai PyPDF2 sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da56731e-681a-4caa-8015-6c3f84fb59b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List\n",
    "from PyPDF2 import PdfReader\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import requests\n",
    "\n",
    "# -----------------------------------------\n",
    "# Setup Logging\n",
    "# -----------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdeb3991-3896-4d92-af4a-504e4df48631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Constants - configuration\n",
    "# -----------------------------------------\n",
    "SAMPLE_PDF_URL = \"https://pdfobject.com/pdf/sample.pdf\"  # URL for a sample PDF file\n",
    "#SAMPLE_PDF_URL = \"https://bugs.python.org/file47781/Tutorial_EDIT.pdf\" # Python tutorial sample\n",
    "PDF_PATH = \"sample.pdf\"  # Path to save or check for the PDF file\n",
    "QUERIES = [\n",
    "    \"What is the main idea of the document?\",\n",
    "    \"Summarize the key topics discussed.\"\n",
    "]\n",
    "CHROMA_DB_DIR = \"./chromadb\"  # Directory for ChromaDB storage\n",
    "COLLECTION_NAME = \"rag_documents\"  # Logical name for the ChromaDB collection\n",
    "CHUNK_SIZE = 500  # Number of characters in each text chunk\n",
    "NUM_CHUNKS = 2  # Number of chunks to retrieve for each query\n",
    "MODEL_TYPE = \"fakellm\"  # Default model (\"chatgpt\", \"ollama\", or \"fakellm\")\n",
    "OLLAMA_ENDPOINT = \"http://localhost:11434/api/generate\"  # Ollama server endpoint\n",
    "DEFAULT_OLLAMA_MODEL = \"granite3-dense\"  # Default Ollama model (2b model)\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")  # Define OPENAI_API_KEY in your ENV\n",
    "CHATGPT_MODEL_NAME = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b16ae45-6016-4064-8863-fa536c617b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Utility Functions\n",
    "# -----------------------------------------\n",
    "def download_sample_pdf(url: str, save_path: str) -> None:\n",
    "    \"\"\"Downloads a sample PDF file.\"\"\"\n",
    "    logger.info(\"Downloading sample PDF...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    logger.info(f\"Sample PDF downloaded to {save_path}\")\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts all text from a PDF file.\"\"\"\n",
    "    logger.info(f\"Extracting text from PDF at {pdf_path}...\")\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    logger.info(\"Text extraction completed.\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int = 500) -> List[str]:\n",
    "    \"\"\"Splits text into smaller chunks.\"\"\"\n",
    "    logger.info(\"Splitting text into chunks...\")\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    logger.info(f\"Text split into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_chroma_client_and_collection(collection_name: str) -> chromadb.api.Collection:\n",
    "    \"\"\"Creates a ChromaDB client and retrieves a collection.\"\"\"\n",
    "    logger.info(\"Setting up ChromaDB client and collection...\")\n",
    "    chroma_client = chromadb.Client(Settings(persist_directory=CHROMA_DB_DIR))\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    logger.info(f\"ChromaDB collection '{collection_name}' created or retrieved.\")\n",
    "    return collection\n",
    "\n",
    "\n",
    "def ingest_pdf_to_chromadb(pdf_path: str, collection: chromadb.api.Collection, chunk_size: int = 500) -> None:\n",
    "    \"\"\"Ingests text from a PDF into a ChromaDB collection.\"\"\"\n",
    "    logger.info(f\"Ingesting PDF from '{pdf_path}' into ChromaDB...\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = split_text_into_chunks(text, chunk_size)\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        if chunk.strip():  # Skip empty chunks\n",
    "            collection.upsert(\n",
    "                documents=[chunk],\n",
    "                ids=[f\"doc_{idx}\"]\n",
    "            )\n",
    "    logger.info(f\"Ingested {len(chunks)} chunks into ChromaDB.\")\n",
    "\n",
    "\n",
    "def query_vector_db(collection: chromadb.api.Collection, query: str, max_results: int = NUM_CHUNKS) -> List[str]:\n",
    "    \"\"\"Queries the vector database for the most similar chunks.\"\"\"\n",
    "    logger.info(f\"Querying vector DB for: '{query}'...\")\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=max_results\n",
    "    )\n",
    "    documents = results[\"documents\"][0]\n",
    "    logger.info(f\"Retrieved {len(documents)} matching chunks from the vector DB.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb598fb1-695a-4a3b-acba-0327620fbe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Query Functions\n",
    "# -----------------------------------------\n",
    "def query_llm(\n",
    "    query: str,\n",
    "    context: List[str],\n",
    "    model_type: str = MODEL_TYPE,\n",
    "    ollama_model: str = DEFAULT_OLLAMA_MODEL\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Queries a language model (LLM) with context.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query.\n",
    "        context (List[str]): Context from ChromaDB.\n",
    "        model_type (str): Model type (\"chatgpt\", \"ollama\", \"fakellm\").\n",
    "        ollama_model (str): Specific Ollama model.\n",
    "\n",
    "    Returns:\n",
    "        str: LLM response.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Querying LLM {model_type} with query: {query} and context: {context}\")\n",
    "    if model_type == \"chatgpt\":\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant for answering questions about PDF documents.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {' '.join(context)}\\n\\nQuestion: {query}\"}\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=CHATGPT_MODEL_NAME,\n",
    "            messages=messages\n",
    "        )\n",
    "        logger.info(\"ChatGPT query completed.\")\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    elif model_type == \"ollama\":\n",
    "        payload = {\n",
    "            \"model\": ollama_model,\n",
    "            \"prompt\": f\"Context: {' '.join(context)}\\n\\nQuestion: {query}\",\n",
    "            \"stream\": False\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(OLLAMA_ENDPOINT, json=payload)\n",
    "            response.raise_for_status()\n",
    "            logger.info(f\"Ollama query completed using model: {ollama_model}.\")\n",
    "            return response.json().get(\"response\", \"No response from Ollama.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ollama query error: {e}\")\n",
    "            raise\n",
    "\n",
    "    elif model_type == \"fakellm\":\n",
    "        logger.info(\"Fake LLM returning input as response.\")\n",
    "        return f\"Context: {' '.join(context)}\\n\\nQuery: {query}\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. Use 'chatgpt', 'ollama', or 'fakellm'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81acff72-4587-4b25-b454-fcee2af0c903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 06:55:13,417 - INFO - PDF not found locally. Downloading...\n",
      "2024-11-17 06:55:13,418 - INFO - Downloading sample PDF...\n",
      "2024-11-17 06:55:13,875 - INFO - Sample PDF downloaded to sample.pdf\n",
      "2024-11-17 06:55:13,875 - INFO - Setting up ChromaDB client and collection...\n",
      "2024-11-17 06:55:13,895 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-11-17 06:55:14,017 - INFO - ChromaDB collection 'rag_documents' created or retrieved.\n",
      "2024-11-17 06:55:14,017 - INFO - Ingesting PDF from 'sample.pdf' into ChromaDB...\n",
      "2024-11-17 06:55:14,018 - INFO - Extracting text from PDF at sample.pdf...\n",
      "2024-11-17 06:55:14,030 - INFO - Text extraction completed.\n",
      "2024-11-17 06:55:14,030 - INFO - Splitting text into chunks...\n",
      "2024-11-17 06:55:14,030 - INFO - Text split into 6 chunks.\n",
      "2024-11-17 06:55:14,384 - INFO - Ingested 6 chunks into ChromaDB.\n",
      "2024-11-17 06:55:14,385 - INFO - Processing query: What is the main idea of the document?\n",
      "2024-11-17 06:55:14,385 - INFO - Querying vector DB for: 'What is the main idea of the document?'...\n",
      "2024-11-17 06:55:14,417 - INFO - Retrieved 2 matching chunks from the vector DB.\n",
      "2024-11-17 06:55:14,417 - INFO - Querying LLM fakellm with query: What is the main idea of the document? and context: ['Sample PDFThis is a simple PDF ﬁle. Fun fun fun.Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla erat dolor, bland', 'rsus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus varius. Ut sit amet diam suscipit mauris ornare aliquam. Sed varius. Duis arcu. Etiam tristique massa eget dui. Phasellus congue. Aenean est erat, tincidunt eget, venenatis quis, commodo at, quam.']\n",
      "2024-11-17 06:55:14,417 - INFO - Fake LLM returning input as response.\n",
      "2024-11-17 06:55:14,418 - INFO - Processing query: Summarize the key topics discussed.\n",
      "2024-11-17 06:55:14,418 - INFO - Querying vector DB for: 'Summarize the key topics discussed.'...\n",
      "2024-11-17 06:55:14,451 - INFO - Retrieved 2 matching chunks from the vector DB.\n",
      "2024-11-17 06:55:14,452 - INFO - Querying LLM fakellm with query: Summarize the key topics discussed. and context: ['Sample PDFThis is a simple PDF ﬁle. Fun fun fun.Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla erat dolor, bland', 'inar, nunc quis iaculis sagittis, justo quam lobortis tortor, sed vestibulum dui metus venenatis est. Nunc cursus ligula. Nulla facilisi. Phasellus ullamcorper consectetuer ante. Duis tincidunt, urna id condimentum luctus, nibh ante vulputate sapien, id sagittis massa orci ut enim. Pellentesque vestibulum convallis sem. Nulla consequat quam ut nisl. Nullam est. Curabitur tincidunt dapibus lorem. Proin velit turpis, scelerisque sit amet, iaculis nec, rhoncus ac, ipsum. Phasellus lorem arcu, feugi']\n",
      "2024-11-17 06:55:14,452 - INFO - Fake LLM returning input as response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the main idea of the document?\n",
      "LLM Response:\n",
      "Context: Sample PDFThis is a simple PDF ﬁle. Fun fun fun.Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla erat dolor, bland rsus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus varius. Ut sit amet diam suscipit mauris ornare aliquam. Sed varius. Duis arcu. Etiam tristique massa eget dui. Phasellus congue. Aenean est erat, tincidunt eget, venenatis quis, commodo at, quam.\n",
      "\n",
      "Query: What is the main idea of the document?\n",
      "\n",
      "Query: Summarize the key topics discussed.\n",
      "LLM Response:\n",
      "Context: Sample PDFThis is a simple PDF ﬁle. Fun fun fun.Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla erat dolor, bland inar, nunc quis iaculis sagittis, justo quam lobortis tortor, sed vestibulum dui metus venenatis est. Nunc cursus ligula. Nulla facilisi. Phasellus ullamcorper consectetuer ante. Duis tincidunt, urna id condimentum luctus, nibh ante vulputate sapien, id sagittis massa orci ut enim. Pellentesque vestibulum convallis sem. Nulla consequat quam ut nisl. Nullam est. Curabitur tincidunt dapibus lorem. Proin velit turpis, scelerisque sit amet, iaculis nec, rhoncus ac, ipsum. Phasellus lorem arcu, feugi\n",
      "\n",
      "Query: Summarize the key topics discussed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Main Workflow\n",
    "# -----------------------------------------\n",
    "def main() -> None:\n",
    "    \"\"\"Main workflow to ingest a PDF, query ChromaDB, and use an LLM.\"\"\"\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        logger.info(\"PDF not found locally. Downloading...\")\n",
    "        download_sample_pdf(SAMPLE_PDF_URL, PDF_PATH)\n",
    "\n",
    "    collection = create_chroma_client_and_collection(COLLECTION_NAME)\n",
    "    ingest_pdf_to_chromadb(PDF_PATH, collection, CHUNK_SIZE)\n",
    "\n",
    "    for query in QUERIES:\n",
    "        logger.info(f\"Processing query: {query}\")\n",
    "        matches = query_vector_db(collection, query)\n",
    "        if not matches:\n",
    "            logger.warning(f\"No matches found for query: {query}\")\n",
    "            print(f\"No matches for query: {query}\\n\")\n",
    "            continue\n",
    "        response = query_llm(query, matches)\n",
    "        print(f\"Query: {query}\\nLLM Response:\\n{response}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72685dfe-941a-44fe-9b49-4bb482c3f5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
