{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f61ebd-c891-4220-a977-9236700ceec7",
   "metadata": {},
   "source": [
    "# PDF Querying with ChromaDB and ChatGPT\n",
    "\n",
    "> Build your own \"Chat with Local Files\" using Retrieval Augmented Generation\n",
    "> Mihai Criveti, PyCon Ireland 2024\n",
    "\n",
    "## Overview\n",
    "\n",
    "This script processes a PDF document, extracts its content, stores it in ChromaDB.\n",
    "It uses a language model (FakeLLM, ChatGPT or Ollama) to generate context-aware responses.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **PDF Ingestion**:\n",
    "   - Downloads a sample PDF if not already available.\n",
    "   - Extracts text from the PDF and splits it into chunks.\n",
    "\n",
    "2. **ChromaDB Setup**:\n",
    "   - Initializes a ChromaDB client and creates or retrieves a collection.\n",
    "   - Stores the text chunks in the ChromaDB collection.\n",
    "\n",
    "3. **Querying**:\n",
    "   - Searches the ChromaDB collection for chunks most similar to the user’s query.\n",
    "   - Passes the top match as context to the selected language model (FakeLLM, ChatGPT or Ollama).\n",
    "\n",
    "4. **Response Generation**:\n",
    "   - The language model generates a detailed response based on the retrieved context and user query.\n",
    "\n",
    "## Workflow Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Start] --> B[Download Sample PDF]\n",
    "    B --> C[Extract Text from PDF]\n",
    "    C --> D[Split Text into Chunks]\n",
    "    D --> E[Initialize ChromaDB Client]\n",
    "    E --> F[Store Chunks in ChromaDB Collection]\n",
    "    F --> G[Query Vector DB with User Query]\n",
    "    G --> H[Retrieve Top Matches]\n",
    "    H --> I[Pass Top Match to LLM]\n",
    "    I --> J[Generate Response]\n",
    "    J --> K[Display Results]\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f601da-2a99-4455-bfcc-481154e3020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install chromadb openai PyPDF2 sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da56731e-681a-4caa-8015-6c3f84fb59b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List\n",
    "from PyPDF2 import PdfReader\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import requests\n",
    "\n",
    "# -----------------------------------------\n",
    "# Setup Logging\n",
    "# -----------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdeb3991-3896-4d92-af4a-504e4df48631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Constants - configuration\n",
    "# -----------------------------------------\n",
    "SAMPLE_PDF_URL = \"https://pdfobject.com/pdf/sample.pdf\"  # URL for a sample PDF file\n",
    "#SAMPLE_PDF_URL = \"https://bugs.python.org/file47781/Tutorial_EDIT.pdf\" # Python tutorial sample\n",
    "PDF_PATH = \"sample.pdf\"  # Path to save or check for the PDF file\n",
    "QUERIES = [\n",
    "    \"What is the main idea of the document?\",\n",
    "    \"Summarize the key topics discussed.\"\n",
    "]\n",
    "CHROMA_DB_DIR = \"./chromadb\"  # Directory for ChromaDB storage\n",
    "COLLECTION_NAME = \"rag_documents\"  # Logical name for the ChromaDB collection\n",
    "CHUNK_SIZE = 500  # Number of characters in each text chunk\n",
    "NUM_CHUNKS = 2  # Number of chunks to retrieve for each query\n",
    "MODEL_TYPE = \"fakellm\"  # Default model (\"chatgpt\", \"ollama\", or \"fakellm\")\n",
    "OLLAMA_ENDPOINT = \"http://localhost:11434/api/generate\"  # Ollama server endpoint\n",
    "DEFAULT_OLLAMA_MODEL = \"granite3-dense\"  # Default Ollama model (2b model)\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")  # Define OPENAI_API_KEY in your ENV\n",
    "CHATGPT_MODEL_NAME = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b16ae45-6016-4064-8863-fa536c617b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Utility Functions\n",
    "# -----------------------------------------\n",
    "def download_sample_pdf(url: str, save_path: str) -> None:\n",
    "    \"\"\"Downloads a sample PDF file.\"\"\"\n",
    "    logger.info(\"Downloading sample PDF...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    logger.info(f\"Sample PDF downloaded to {save_path}\")\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts all text from a PDF file.\"\"\"\n",
    "    logger.info(f\"Extracting text from PDF at {pdf_path}...\")\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    logger.info(\"Text extraction completed.\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int = 500) -> List[str]:\n",
    "    \"\"\"Splits text into smaller chunks.\"\"\"\n",
    "    logger.info(\"Splitting text into chunks...\")\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    logger.info(f\"Text split into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_chroma_client_and_collection(collection_name: str) -> chromadb.api.Collection:\n",
    "    \"\"\"Creates a ChromaDB client and retrieves a collection.\"\"\"\n",
    "    logger.info(\"Setting up ChromaDB client and collection...\")\n",
    "    chroma_client = chromadb.Client(Settings(persist_directory=CHROMA_DB_DIR))\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    logger.info(f\"ChromaDB collection '{collection_name}' created or retrieved.\")\n",
    "    return collection\n",
    "\n",
    "\n",
    "def ingest_pdf_to_chromadb(pdf_path: str, collection: chromadb.api.Collection, chunk_size: int = 500) -> None:\n",
    "    \"\"\"Ingests text from a PDF into a ChromaDB collection.\"\"\"\n",
    "    logger.info(f\"Ingesting PDF from '{pdf_path}' into ChromaDB...\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = split_text_into_chunks(text, chunk_size)\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        if chunk.strip():  # Skip empty chunks\n",
    "            collection.upsert(\n",
    "                documents=[chunk],\n",
    "                ids=[f\"doc_{idx}\"]\n",
    "            )\n",
    "    logger.info(f\"Ingested {len(chunks)} chunks into ChromaDB.\")\n",
    "\n",
    "\n",
    "def query_vector_db(collection: chromadb.api.Collection, query: str, max_results: int = NUM_CHUNKS) -> List[str]:\n",
    "    \"\"\"Queries the vector database for the most similar chunks.\"\"\"\n",
    "    logger.info(f\"Querying vector DB for: '{query}'...\")\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=max_results\n",
    "    )\n",
    "    documents = results[\"documents\"][0]\n",
    "    logger.info(f\"Retrieved {len(documents)} matching chunks from the vector DB.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb598fb1-695a-4a3b-acba-0327620fbe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Query Functions\n",
    "# -----------------------------------------\n",
    "def query_llm(\n",
    "    query: str,\n",
    "    context: List[str],\n",
    "    model_type: str = MODEL_TYPE,\n",
    "    ollama_model: str = DEFAULT_OLLAMA_MODEL\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Queries a language model (LLM) with context.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query.\n",
    "        context (List[str]): Context from ChromaDB.\n",
    "        model_type (str): Model type (\"chatgpt\", \"ollama\", \"fakellm\").\n",
    "        ollama_model (str): Specific Ollama model.\n",
    "\n",
    "    Returns:\n",
    "        str: LLM response.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Querying LLM {model_type} with query: {query} and context: {context}\")\n",
    "    if model_type == \"chatgpt\":\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant for answering questions about PDF documents.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {' '.join(context)}\\n\\nQuestion: {query}\"}\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=CHATGPT_MODEL_NAME,\n",
    "            messages=messages\n",
    "        )\n",
    "        logger.info(\"ChatGPT query completed.\")\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    elif model_type == \"ollama\":\n",
    "        payload = {\n",
    "            \"model\": ollama_model,\n",
    "            \"prompt\": f\"Context: {' '.join(context)}\\n\\nQuestion: {query}\",\n",
    "            \"stream\": False\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(OLLAMA_ENDPOINT, json=payload)\n",
    "            response.raise_for_status()\n",
    "            logger.info(f\"Ollama query completed using model: {ollama_model}.\")\n",
    "            return response.json().get(\"response\", \"No response from Ollama.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ollama query error: {e}\")\n",
    "            raise\n",
    "\n",
    "    elif model_type == \"fakellm\":\n",
    "        logger.info(\"Fake LLM returning input as response.\")\n",
    "        return f\"Context: {' '.join(context)}\\n\\nQuery: {query}\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. Use 'chatgpt', 'ollama', or 'fakellm'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81acff72-4587-4b25-b454-fcee2af0c903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 06:43:28,882 - INFO - Setting up ChromaDB client and collection...\n",
      "2024-11-17 06:43:28,887 - INFO - ChromaDB collection 'rag_documents' created or retrieved.\n",
      "2024-11-17 06:43:28,887 - INFO - Ingesting PDF from 'sample.pdf' into ChromaDB...\n",
      "2024-11-17 06:43:28,888 - INFO - Extracting text from PDF at sample.pdf...\n",
      "2024-11-17 06:43:28,902 - INFO - Text extraction completed.\n",
      "2024-11-17 06:43:28,903 - INFO - Splitting text into chunks...\n",
      "2024-11-17 06:43:28,903 - INFO - Text split into 6 chunks.\n",
      "2024-11-17 06:43:29,159 - INFO - Ingested 6 chunks into ChromaDB.\n",
      "2024-11-17 06:43:29,160 - INFO - Processing query: What is the main idea of the document?\n",
      "2024-11-17 06:43:29,160 - INFO - Querying vector DB for: 'What is the main idea of the document?'...\n",
      "2024-11-17 06:43:29,193 - INFO - Retrieved 2 matching chunks from the vector DB.\n",
      "2024-11-17 06:43:29,194 - INFO - Querying LLM fakellm with query: What is the main idea of the document? and context: ['Sample PDFThis is a simple PDF ﬁle. Fun fun fun.Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla erat dolor, bland', 'rsus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus varius. Ut sit amet diam suscipit mauris ornare aliquam. Sed varius. Duis arcu. Etiam tristique massa eget dui. Phasellus congue. Aenean est erat, tincidunt eget, venenatis quis, commodo at, quam.']\n",
      "2024-11-17 06:43:29,194 - INFO - Fake LLM returning input as response.\n",
      "2024-11-17 06:43:29,195 - INFO - Processing query: Summarize the key topics discussed.\n",
      "2024-11-17 06:43:29,195 - INFO - Querying vector DB for: 'Summarize the key topics discussed.'...\n",
      "2024-11-17 06:43:29,228 - INFO - Retrieved 2 matching chunks from the vector DB.\n",
      "2024-11-17 06:43:29,228 - INFO - Querying LLM fakellm with query: Summarize the key topics discussed. and context: ['Sample PDFThis is a simple PDF ﬁle. Fun fun fun.Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla erat dolor, bland', 'inar, nunc quis iaculis sagittis, justo quam lobortis tortor, sed vestibulum dui metus venenatis est. Nunc cursus ligula. Nulla facilisi. Phasellus ullamcorper consectetuer ante. Duis tincidunt, urna id condimentum luctus, nibh ante vulputate sapien, id sagittis massa orci ut enim. Pellentesque vestibulum convallis sem. Nulla consequat quam ut nisl. Nullam est. Curabitur tincidunt dapibus lorem. Proin velit turpis, scelerisque sit amet, iaculis nec, rhoncus ac, ipsum. Phasellus lorem arcu, feugi']\n",
      "2024-11-17 06:43:29,229 - INFO - Fake LLM returning input as response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the main idea of the document?\n",
      "LLM Response:\n",
      "Context: Sample PDFThis is a simple PDF ﬁle. Fun fun fun.Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla erat dolor, bland rsus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus varius. Ut sit amet diam suscipit mauris ornare aliquam. Sed varius. Duis arcu. Etiam tristique massa eget dui. Phasellus congue. Aenean est erat, tincidunt eget, venenatis quis, commodo at, quam.\n",
      "\n",
      "Query: What is the main idea of the document?\n",
      "\n",
      "Query: Summarize the key topics discussed.\n",
      "LLM Response:\n",
      "Context: Sample PDFThis is a simple PDF ﬁle. Fun fun fun.Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla erat dolor, bland inar, nunc quis iaculis sagittis, justo quam lobortis tortor, sed vestibulum dui metus venenatis est. Nunc cursus ligula. Nulla facilisi. Phasellus ullamcorper consectetuer ante. Duis tincidunt, urna id condimentum luctus, nibh ante vulputate sapien, id sagittis massa orci ut enim. Pellentesque vestibulum convallis sem. Nulla consequat quam ut nisl. Nullam est. Curabitur tincidunt dapibus lorem. Proin velit turpis, scelerisque sit amet, iaculis nec, rhoncus ac, ipsum. Phasellus lorem arcu, feugi\n",
      "\n",
      "Query: Summarize the key topics discussed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Main Workflow\n",
    "# -----------------------------------------\n",
    "def main() -> None:\n",
    "    \"\"\"Main workflow to ingest a PDF, query ChromaDB, and use an LLM.\"\"\"\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        logger.info(\"PDF not found locally. Downloading...\")\n",
    "        download_sample_pdf(SAMPLE_PDF_URL, PDF_PATH)\n",
    "\n",
    "    collection = create_chroma_client_and_collection(COLLECTION_NAME)\n",
    "    ingest_pdf_to_chromadb(PDF_PATH, collection, CHUNK_SIZE)\n",
    "\n",
    "    for query in QUERIES:\n",
    "        logger.info(f\"Processing query: {query}\")\n",
    "        matches = query_vector_db(collection, query)\n",
    "        if not matches:\n",
    "            logger.warning(f\"No matches found for query: {query}\")\n",
    "            print(f\"No matches for query: {query}\\n\")\n",
    "            continue\n",
    "        response = query_llm(query, matches)\n",
    "        print(f\"Query: {query}\\nLLM Response:\\n{response}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
